import streamlit as st
import pandas as pd
import difflib
import re
import Levenshtein
from openai import OpenAI
from streamlit_mic_recorder import mic_recorder
import random

# --- 0. í˜ì´ì§€ ì„¤ì • ---
st.set_page_config(layout="wide", page_title="Pronko - AI Pronunciation Analyzer")

# --- 1. ë‹¤êµ­ì–´ í…ìŠ¤íŠ¸ ë° ìƒíƒœ ê´€ë¦¬ ---
def initialize_state():
    """ì„¸ì…˜ ìƒíƒœë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    if 'lang' not in st.session_state:
        st.session_state.lang = 'ko'
    if 'prompt' not in st.session_state:
        st.session_state.prompt = ""
    if 'guide_audio' not in st.session_state:
        st.session_state.guide_audio = None

TEXTS = {
    "ko": {
        "title": "AI í•œêµ­ì–´ ë°œìŒ ë¶„ì„ê¸°", "lang_select": "ì–¸ì–´ ì„ íƒ", "header": "ë°œìŒ ì—°ìŠµ", 
        "prompt_source": "ì—°ìŠµ ë¬¸ì¥ ì„ íƒ", "source_random": "ëœë¤ ë¬¸ì¥", "source_gpt": "AI ìƒì„±",
        "new_sentence_button": "ìƒˆë¡œìš´ ëœë¤ ë¬¸ì¥",
        "start_prompt": "ğŸ¤ ë…¹ìŒ ì‹œì‘", "stop_prompt": "â¹ï¸ ë…¹ìŒ ì¤‘ì§€", "result_header": "ìŒì„± ë¶„ì„ ê²°ê³¼", 
        "my_audio": "ë‚´ ë…¹ìŒ ë‹¤ì‹œ ë“£ê¸°", "accuracy": "ë°œìŒ ì •í™•ë„", "compare_header": "ìƒì„¸ ë¹„êµ ë¶„ì„", 
        "compare_std": "ëª©í‘œ ë°œìŒ", "compare_pred": "ë‚´ ë°œìŒ ì¸ì‹ ê²°ê³¼ (AI)", 
        "error_api_key": "OpenAI API í‚¤ë¥¼ .streamlit/secrets.tomlì— ì„¤ì •í•´ì£¼ì„¸ìš”.",
        "spinner_tts": "ê°€ì´ë“œ ìŒì„±ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...", "spinner_stt": "AIê°€ ë‹¹ì‹ ì˜ ë°œìŒì„ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...",
    },
    "en": {
        "title": "AI Korean Pronunciation Analyzer", "lang_select": "Select Language", "header": "Pronunciation Practice", 
        "prompt_source": "Choose a practice sentence", "source_random": "Random Sentence", "source_gpt": "AI Generated",
        "new_sentence_button": "New Random Sentence",
        "start_prompt": "â–¶ï¸ Start Recording", "stop_prompt": "â¹ï¸ Stop Recording", "result_header": "Voice Analysis Result", 
        "my_audio": "Listen to My Recording", "accuracy": "Pronunciation Accuracy", "compare_header": "Detailed Comparison",
        "compare_std": "Target Pronunciation", "compare_pred": "My Pronunciation (AI Recognized)", 
        "error_api_key": "Please set up your OpenAI API key in .streamlit/secrets.toml",
        "spinner_tts": "Generating guide audio...", "spinner_stt": "AI is analyzing your pronunciation...",
    }
}

# --- 2. í•µì‹¬ ë¡œì§ í•¨ìˆ˜ë“¤ ---

@st.cache_resource
def get_openai_client():
    """OpenAI í´ë¼ì´ì–¸íŠ¸ ê°ì²´ë¥¼ ìƒì„±í•˜ê³  ìºì‹œí•©ë‹ˆë‹¤."""
    try:
        return OpenAI(api_key=st.secrets["OPENAI_API_KEY"])
    except (KeyError, FileNotFoundError):
        st.error(TEXTS[st.session_state.get('lang', 'ko')]["error_api_key"])
        st.stop()

@st.cache_data
def get_practice_sentences():
    """ë¯¸ë¦¬ ì •ì˜ëœ ì—°ìŠµ ë¬¸ì¥ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    return [
        "ë‚˜ëŠ” ì§‘ ë‚´ë¶€ ê³µì‚¬ë¥¼ ëëƒˆë‹¤.", "ì•„ì¹¨ì— ì•„ë¬´ê²ƒë„ ë¨¹ì§€ ì•ŠëŠ” ì‚¬ëŒë“¤ì´ ë§ìŠµë‹ˆë‹¤.",
        "ë„ˆëŠ” í´ë˜ì‹ ìŒì•… ë“£ëŠ” ê±¸ ì¢‹ì•„í•˜ì§€, ê·¸ë ‡ì§€?", "ìƒì„ ì„ ë¨¹ë˜ ê³ ì–‘ì´ê°€ ê°•ì•„ì§€í•œí…Œ ì«“ê²¼ë‹¤.",
    ]

def preprocess_text(text: str) -> str:
    """CER ê³„ì‚°ì„ ìœ„í•´ í…ìŠ¤íŠ¸ì—ì„œ ê³µë°±ê³¼ íŠ¹ìˆ˜ë¬¸ìë¥¼ ì œê±°í•©ë‹ˆë‹¤."""
    text = text.lower().strip()
    return re.sub(r"[^a-z0-9ê°€-í£]", "", text)

def calculate_cer(prediction: str, standard: str) -> float:
    """ë‘ í…ìŠ¤íŠ¸ì˜ ê¸€ì ì˜¤ë¥˜ìœ¨(CER)ì„ ê³„ì‚°í•©ë‹ˆë‹¤."""
    pred_clean = preprocess_text(prediction)
    std_clean = preprocess_text(standard)
    distance = Levenshtein.distance(pred_clean, std_clean)
    std_len = len(std_clean)
    return round(distance / std_len, 4) if std_len > 0 else 0.0

# â­ï¸ ìºì‹œ ì œê±°: ë…¹ìŒí•  ë•Œë§ˆë‹¤ í•­ìƒ ìƒˆë¡œ ë¶„ì„í•´ì•¼ í•¨
def analyze_with_whisper(_client, _audio_bytes):
    """Whisper APIë¥¼ í˜¸ì¶œí•˜ì—¬ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    with st.spinner(TEXTS[st.session_state.lang]['spinner_stt']):
        try:
            audio_file = ("audio.wav", _audio_bytes, "audio/wav")
            transcription = _client.audio.transcriptions.create(model="whisper-1", file=audio_file, response_format="text")
            return transcription
        except Exception as e:
            st.error(f"{TEXTS[st.session_state.lang]['error_api_call']} {e}")
            return None

# @st.cache_data
def generate_tts(_client, _text):
    """OpenAI TTS APIë¥¼ í˜¸ì¶œí•˜ì—¬ ê°€ì´ë“œ ìŒì„±ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    with st.spinner(TEXTS[st.session_state.lang]['spinner_tts']):
        try:
            response = _client.audio.speech.create(model="tts-1", voice="nova", input=_text)
            return response.content
        except Exception as e:
            st.error(f"{TEXTS[st.session_state.lang]['error_api_call']} {e}")
            return None

def render_diff_comparison(std_text: str, pred_text: str, T):
    """
    [ìµœì¢… ì‹œê°í™” í•¨ìˆ˜]
    ëª©í‘œ ë°œìŒì€ ê·¸ëŒ€ë¡œ, ì˜ˆì¸¡ ë°œìŒì—ì„œë§Œ ë§ê³  í‹€ë¦° ë¶€ë¶„ì„ ìƒ‰ìƒìœ¼ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.
    """
    matcher = difflib.SequenceMatcher(None, std_text, pred_text)
    
    styled_prediction = ""
    # ëª¨ë˜í•œ ìƒ‰ìƒ íŒ”ë ˆíŠ¸
    match_color = "#198754"  # ì¼ì¹˜: ì„ ëª…í•œ ì´ˆë¡ìƒ‰ (Bootstrap Success)
    error_color = "#dc3545"  # ë¶ˆì¼ì¹˜: ì„ ëª…í•œ ë¹¨ê°„ìƒ‰ (Bootstrap Danger)
    
    for tag, i1, i2, j1, j2 in matcher.get_opcodes():
        if tag == 'equal':
            styled_prediction += f'<span style="color: {match_color}; font-weight: 500;">{pred_text[j1:j2]}</span>'
        else:
            styled_prediction += f'<span style="color: {error_color}; font-weight: 700; text-decoration: underline; text-decoration-style: wavy;">{pred_text[j1:j2]}</span>'

    # UI ë Œë”ë§
    style = "font-size: 1.2rem; padding: 15px; border: 1px solid #dee2e6; border-radius: 8px; line-height: 2.0;"
    
    st.markdown(f"**{T['compare_std']}**")
    st.markdown(f'<div style="{style}">{std_text}</div>', unsafe_allow_html=True)
    
    st.markdown(f"**{T['compare_pred']}**")
    st.markdown(f'<div style="{style}">{styled_prediction}</div>', unsafe_allow_html=True)

# --- 3. ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰ ---
# --- 3. ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰ ---

# 1. ì´ˆê¸°í™”
initialize_state()
client = get_openai_client()

# 2. ì‚¬ì´ë“œë°” (ì–¸ì–´ ì„ íƒ)
with st.sidebar:
    st.title("Settings")
    # ì–¸ì–´ê°€ ë³€ê²½ë˜ë©´, promptì™€ audioë¥¼ ì´ˆê¸°í™”í•˜ê³  ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¬ì‹¤í–‰
    if st.radio("Language", ["ko", "en"], format_func=lambda x: "í•œêµ­ì–´" if x == "ko" else "English", key="lang_selector") != st.session_state.lang:
        st.session_state.lang = "en" if st.session_state.lang == "ko" else "ko"
        st.session_state.prompt = ""
        st.session_state.guide_audio = None
        st.rerun()

T = TEXTS[st.session_state.lang]

# 3. ë©”ì¸ í˜ì´ì§€ ë ˆì´ì•„ì›ƒ
st.title(T["title"])
st.header(T["header"])

# 4. ë¬¸ì¥ ì„ íƒ UI
source_option = st.radio(
    T["prompt_source"],
    options=["random", "gpt"],
    format_func=lambda x: T["source_random"] if x == "random" else T["source_gpt"],
    horizontal=True,
    key="source_option_radio"
)

if source_option == "random":
    if st.button(T["new_sentence_button"], key="new_random_sentence_button"):
        st.session_state.prompt = random.choice(get_practice_sentences())
        st.session_state.guide_audio = None # ìºì‹œ ë¬´íš¨í™”
        st.rerun()
else: # GPT
    with st.form(key="gpt_form"):
        topic = st.text_input(TEXTS[st.session_state.lang]["gpt_placeholder"], key="gpt_topic_input")
        submitted = st.form_submit_button(TEXTS[st.session_state.lang]["gpt_prompt_button"])
        if submitted and topic:
            new_prompt = generate_sentence_with_gpt(client, topic, st.session_state.lang)
            if new_prompt:
                st.session_state.prompt = new_prompt
                st.session_state.guide_audio = None
                st.rerun()

# 5. í˜„ì¬ ì—°ìŠµ ë¬¸ì¥ ë° ê°€ì´ë“œ ì˜¤ë””ì˜¤ í‘œì‹œ
# ìµœì´ˆ ì‹¤í–‰ ì‹œ ë˜ëŠ” ë¬¸ì¥ì´ ì—†ì„ ë•Œ ì´ˆê¸° ë¬¸ì¥ ì„¤ì •
if not st.session_state.prompt:
    st.session_state.prompt = get_practice_sentences()[0]
    st.session_state.guide_audio = None # ì˜¤ë””ì˜¤ë„ í•¨ê»˜ ì´ˆê¸°í™”

# TTS ì˜¤ë””ì˜¤ ìƒì„± ë° ìºì‹± ë¡œì§
if st.session_state.guide_audio is None:
    audio_content = generate_tts(client, st.session_state.prompt)
    st.session_state.guide_audio = audio_content
    
st.subheader(st.session_state.prompt, divider='rainbow')
if st.session_state.guide_audio:
    st.audio(st.session_state.guide_audio, format="audio/mp3")

# 6. ë§ˆì´í¬ ë…¹ìŒ ë° ë¶„ì„
audio_info = mic_recorder(
    start_prompt=T["start_prompt"],
    stop_prompt=T["stop_prompt"],
    just_once=True,
    key='mic_recorder_widget'
)

if audio_info and audio_info['bytes']:
    st.header(T["result_header"], divider='rainbow')
    
    col_listen, col_score = st.columns([1, 2])
    
    with col_listen:
        st.markdown(f"**{T['my_audio']}**")
        st.audio(audio_info['bytes'])
        
    predicted_text = analyze_with_whisper(client, audio_info['bytes'])
    
    if predicted_text:
        cer_score = calculate_cer(predicted_text, st.session_state.prompt)
        accuracy = max(0, (1 - cer_score)) * 100
        
        with col_score:
            st.metric(label=f"**{T['accuracy']}**", value=f"{accuracy:.1f} %",
                      help="ê¸€ì ì˜¤ë¥˜ìœ¨(CER)ì„ ê¸°ë°˜ìœ¼ë¡œ ê³„ì‚°ëœ ì •í™•ë„ì…ë‹ˆë‹¤.")

        # ìµœì¢… ì‹œê°í™” í•¨ìˆ˜ í˜¸ì¶œ
        render_diff_comparison(st.session_state.prompt, predicted_text, T)